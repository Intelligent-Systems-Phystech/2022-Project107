\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\newcommand*{\defeq}{\stackrel{\mathsmaller{\mathsf{def}}}{=}}

\usepackage{hyperref}
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor}

\hypersetup{
	unicode=true,
	pdftitle={A template for the arxiv style},
	pdfsubject={q-bio.NC, q-bio.QM},
	pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
	pdfkeywords={First keyword, Second keyword, More},
	colorlinks=true,
	linkcolor=black,        % внутренние ссылки
	citecolor=blue,         % на библиографию
	filecolor=magenta,      % на файлы
	urlcolor=blue           % на URL
}




\title{Compression for Federated Random Reshuffling}

\author{ Tikhon Antyshev\\
	MIPT\\
	\And
	Grigory Malinovsky \\
	KAUST\\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arrxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	Federated Random Reshuffling (FedRR) is a recently developed method for federated training of supervised machine learning models via empirical risk minimization. It utilizes Random Reshuffling (RR), a variant of Stochastic Gradient Descent (SGD)along with Local Training carried out by the clients. We propose integration of compression techniques in FedRR, reducing the number of communicated bits in order to overcome communication bottleneck, furthermore we integrate server-side optimization (Server Stepsizes) to get improvement in theory and practice. To the best of our knowledge, this is the first time FedRR will be combined with Server Stepsizes and Compressed Iterates at the same time. 

\end{abstract}


\keywords{Machine Learning \and Federated Learning \and Random Reshuffling }

\section{Introduction}
%\lipsum[2]
%\lipsum[3]
Modern machine learning models heavily rely on Empirical Risk Minimization for supervised training training. The  success of this approach itself can be attributed to a plentiful amount of data available and exponential increase in computing power. The latter remains a problem in machine learning tasks, as some models require ridiculous resources for training. One of the proposed solutions to this is parallel computing - distribution of the tasks between clients. It is known as Federated Learning.

\subsection{Federated Learning}
Federated Learning (FL) is a method of training models, which avoids centralized data-storing and instead relies on clients for greater energy efficiency and privacy. The clients do not share their private data between each other or the server, however each of them partakes in model training by local optimization over personal data.

\subsection{Problem Statement}
We consider the standard finite-sum optimization formulation of
federated learning problem:
\begin{equation}
    x_* = \arg\min_{x \in \mathbb{R}^d}\big[f(x) \defeq \frac{1}{M}\sum_{m=1}^M f_m(x) \big]
    \end{equation}
    \[f_m(x) \defeq \frac{1}{n}\sum_{i = 1}^n f_m^i(x)\]
Where M is number of clients in current task, $f_m:\mathbb{R}^d \rightarrow \mathbb{R}$ is the loss of the model $x$ on the training data subset owned by client or device $m$, which has the finite-sum structure, comprised of $f_m^i: \mathbb{R}^d \rightarrow \mathbb{R}$ - loss of model $x$ on training point $i \in \overline{1, n}$. We shall also assume that $ \forall \; m,\, i:\: f_m^i$ is differentiable and consider strongly convex, convex and non-convex modes.

\subsection{Improving FL}

\textbf{Compressed Communication.} As the model size increases, so does the time required to transfer iteration of parameters. The end result is performance worsening drastically. In order to overcome this communication bottleneck, we will be using gradient compression scheme and compressed iterates.
\par
\textbf{Random Reshuffling.} It has been experimentally proven that using a random permutation of data set provides better results, which is distinct from standard SGD, which picks data points in an unbiased way.
\par
\textbf{Local training.} This can be seen as minibatch SGD performed on the latest version of model, provided by the server.
\par
\textbf{Partial Participation.} FL process consists of communication rounds, in each server chooses a subset of clients to participate in optimization. This is done in order to take limited server capability or limited client availability.
\par
\textbf{Server Stepsizes.} After clients finish local optimization they send their results to the server. The most simple action that can be performed is taking average as new global model state. However, empirical evidence suggests that it is more beneficial to treat received model states as gradient information and performing one step of GD over them.


\section{Contributions}
\label{sec:headings}

%\lipsum[4] See Section \ref{sec:headings}.

%\subsection{Headings: second level}
%\lipsum[5]
%\begin{equation}
%	\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
%\end{equation}

%\subsubsection{Headings: third level}
%\lipsum[6]

%\paragraph{Paragraph}
%\lipsum[7]
\section{Preliminaries}


\section{Theory}
\begin{figure}[h]
\centering
{\includegraphics[scale=0.3]{figures/ServerStep.png} \\ Server Step-Sizes}
\end{figure}


\section{Experiments}
The goal of the experiment is to demonstrate improvements of implementing Compressed Iterates with Server Step-Sizes method. We
run our experiment on $l2$-regularized logistic regression
with the ‘a1a’ dataset from LibSVM.
As a baseline solution we will be demonstrating the results of Proximal and Federated RR. Afterwards, we shall compare performance for proposed algorithm and versions that use either only Server Step-sizes or only Compressed Iterates.
\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale=0.35]{figures/fedVSsgd_a1a.png} \\ Local SGD and Federated RR}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[scale=0.5]{figures/RestNest.png} \\ Performance of Nesterov method}
\end{minipage}
\end{figure}

%
%\begin{figure}[h]
%\begin{minipage}[h]{0.49\linewidth}
%\center{\includegraphics[scale=0.35]{figures/fedVSsgd_a1a.png} \\ Data %distribution before SMOTE}
%\end{minipage}
%\hfill
%\begin{minipage}[h]{0.49\linewidth}
%\center{\includegraphics[scale=0.35]{synth_data_after_smote.png} \\ %%Data distribution after SMOTE}
%\end{minipage}
%\end{figure}


\section{Examples of citations, figures, tables, references}
%\label{sec:others}

\subsection{Citations}
%Citations use \verb+natbib+. The documentation may be found at
%\begin{center}
	%\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
%\end{center}

%Here is an example usage of the two main commands (\verb+citet+ and %\verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
%\lipsum[10]
%See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
%\lipsum[11]

%\begin{figure}
%	\centering
%	\includegraphics[width=0.5\textwidth]{../figures/log_reg_cs_exp.eps}
%	\caption{Sample figure caption.}
%	\label{fig:fig1}
%\end{figure}

\subsection{Tables}
%See awesome Table~\ref{tab:table}.

%The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
%\begin{center}
%	\url{https://www.ctan.org/pkg/booktabs}
%\end{center}


%\begin{table}
%	\caption{Sample table title}
%	\centering
%	\begin{tabular}{lll}
%		\toprule
%		\multicolumn{2}{c}{Part}                   \\
%		\cmidrule(r){1-2}
%		Name     & Description     & Size ($\mu$m) \\
%		\midrule
%		Dendrite & Input terminal  & $\sim$100     \\
%		Axon     & Output terminal & $\sim$10      \\
%		Soma     & Cell body       & up to $10^6$  \\
%		\bottomrule
%	\end{tabular}
%	\label{tab:table}
%\end{table}

\subsection{Lists}
%\begin{itemize}
	%\item Lorem psum dolor sit amet
	%\item consectetur adipiscing elit.
	%\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac %rutrum magna.
%\end{itemize}
\nocite{*}

\bibliographystyle{unsrtnat}
\bibliography{references.bib}

\end{document}



